{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgdSIhvG46EUsGf/UoToQ6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tqZscojW7awO"},"outputs":[],"source":["\n","import os, glob, textwrap\n","from pypdf import PdfReader\n","from sentence_transformers import SentenceTransformer\n","import numpy as np\n","\n","PDF_DIR = \"./pdfs\"\n","CHUNK_SIZE = 800\n","CHUNK_OVERLAP = 200\n","TOP_K = 3\n","\n","# ---- Load and chunk PDFs ----\n","def extract_chunks(path):\n","    reader = PdfReader(path)\n","    chunks = []\n","    for i, page in enumerate(reader.pages, start=1):\n","        text = page.extract_text() or \"\"\n","        text = \" \".join(text.split())\n","        start = 0\n","        while start < len(text):\n","            end = min(len(text), start + CHUNK_SIZE)\n","            chunk = text[start:end]\n","            if chunk.strip():\n","                chunks.append((os.path.basename(path), i, chunk))\n","            start = end - CHUNK_OVERLAP\n","            if start < 0: start = 0\n","    return chunks\n","\n","docs = []\n","for pdf in glob.glob(os.path.join(PDF_DIR, \"*.pdf\")):\n","    docs.extend(extract_chunks(pdf))\n","\n","print(f\"Loaded {len(docs)} text chunks from {len(glob.glob(PDF_DIR+'/*.pdf'))} PDFs.\")\n","\n","# ---- Build embeddings ----\n","model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","embeddings = model.encode([c[2] for c in docs], normalize_embeddings=True)\n","\n","# ---- Q&A loop ----\n","print(\"\\nAsk questions about your PDFs (type 'exit' to quit):\\n\")\n","while True:\n","    q = input(\"Q> \").strip()\n","    if q.lower() in {\"exit\", \"quit\"}:\n","        break\n","    q_emb = model.encode([q], normalize_embeddings=True)[0]\n","    sims = np.dot(embeddings, q_emb)\n","    top_idx = np.argsort(-sims)[:TOP_K]\n","\n","    print(\"\\nAnswer (based on retrieved text):\\n\")\n","    for i in top_idx:\n","        fname, page, chunk = docs[i]\n","        print(f\"[{fname} p.{page}] {textwrap.shorten(chunk, width=200)}\\n\")\n"]}]}